#!/bin/bash
# Author: bo.zhang
currentVersion=V2.1.2
updateDate='2020-05-29'
echo "Version: $currentVersion"
echo "Update: $updateDate"
#2.1.2
##修改存储心跳网络强检查，针对需要写存储心跳网络的集群做检查,增加25、26位掩码检查（check_mn.sh）
##增加bond网卡检查（check_host.sh）
##增加CPU C State检查(check_host.sh)
##增加CPU 温度检查(check_host.sh)
##增加单个OSD的使用率检查(check_ceph.sh)
##增加数据库密码是否正确检查(check_password.sh)
##增加物理机的密码是否为弱密码、是否低于9位检查(check_password.sh)

#2.1.1
##增加存储心跳网络强检查，针对需要写存储心跳网络的集群做检查

#2.1.0
#增加双因子验证登陆优化
#集群prometheus检查

#2.0.0
#增加告警程度的分级
#时间同步的详细节点
#双管理节点数据库备份的同步
#主存储备份存储的挂载检查
#去掉log层级目录
#加密巡检日志
#灾备服务器的容量使用报警，
#亲和组的云主机


#V1.9.0
#1、管理节点改进
#（1）sharedblock置备模式
#（2）增值模块检查；license过期信息
#（3）管理节点数据库异地备份，巡检脚本里面检查异地备份为空
#（4）优化物理机qemu-kvm信息检查不正确
#（5）管理节点修改ssh端口问题
#（6）优化已停止的云主机不显示；
#（7）优化之前版本zsha2检查项
#（8）检测平台是否使用了OSPF功能
#（9）系统盘所使用的盘符查询
#2、物理机检查
#（1）巡检抓取 lvmdump 的输出，
#（2）获取GPU 及vGPU使用情况
#（3）物理机的sysctl 内核参数配置
#（4）每个节点 iptables 信息	
#（5）每个节点ebtables 信息
# 3、告警
# (1)新增ceph osd告警
# (2)新增ceph -s告警
# (3)新增灾备告警
# (4)新增严重告警直接打印
# (5)新增远端备份未生效告警

# V1.8.1
# 修复了Ceph检查完成后提示再次检查的错误
# 新增物理机qemu-kvm进程、virsh list信息打印
# V1.8.0
# 2019-06-11
#1、基本信息获取更新（PM）
#（1）修复了双管理节点检查错误的问题
#（2）修复了集群挂载主存储类别数错误的问题
#（3）修复了备份存储（镜像服务器）总容量/已用容量百分比错误的问题
#（4）修复了IPv6检查错误的问题
#（5）优化了iscsi云盘数量检查、云主机设置高可用检查的速度
#（6）新增检查主存储虚拟总容量/已用容量百分比
#2、物理机检查
#（1）新增管理节点系统盘小于600GB时，提示修改全局设置prometheus保存周期及采样周期
#3、管理节点检查
#（1）增加云主机系统盘和数据云盘容量总和
#（2）按Windows平台、WindowsVirtio平台、Linux平台统计每个平台的云主机数量
#（3）修复镜像服务器检查不正确的问题
#（4）增加检查主存储总容量、已用容量百分比
#（5）按类型统计镜像数量
#（6）neverstop云主机的CPU 内存资源消耗总量
#（7）单个物理机上VM数量超过20 报警
#（8）云主机里面的规格Top 10 排序
#（9）云主机标签数量
#（10）亲和组数量
#（11）查询全局设置自定义的配置项，方便审计
#（12）安全组数量，每个云主机的安全组数量，Top 10排序
#（13）云平台负载均衡器的数量，每监听器的端口， Top 10排序
#（14）云主机GPU设备数量 USB设备数量 块设备数量
#（15）云平台的报警器是否有异常触发
#（16）灾备数据的总大小；灾备任务的数量；灾备容量排序
#（17）Vcenter  几个VC  每个VC 多少云主机， VC的存储容量
#（18）计费单价参考
#（19）企业管理 多少项目 多少用户
#（20）多少裸金属主机
#（21）报警使用了多少邮箱服务器
#（22）灾备数据的总大小；灾备任务的数量；灾备容量排序（资源）备份服务器数量
#（23）扁平网络使用弹性IP的数量
#4、最佳实践
#（1）全局设置物理机保留内存小于8G即报警
#（2）新增对快照数量超过10个的资源进行报警
#（3）检查SharedBlock主存储Lun的个数，若大于1即报警
#（4）主存储使用容量大于65%进行报警
#（5）存储心跳网络检查
#（6）迁移网络检查
#（7）管理节点系统盘小于600GB，报警并提示：全局设置监控数据保留周期设置为1个月，监控数据采样时间间隔设置为20秒
#（8）主存储保留阈值大于0.8即报警
#（9）主存储保留容量低于200GB即报警
#（10）未配置自定义接收端即报警（不含平台HTTP接收端）

# V1.7.2
# 2019-03-26
# 1、新增检查管理节点高可用方案；
# 2、新增管理节点是否与计算节点或镜像服务器复用；
# 3、Ceph是否为超融合部署；
# 4、检查zstack.properties是否配置了固定的时间同步源；
# 5、添加Ceph类型的主存储、镜像服务器时，是否填写了唯一的pool UUID；
# 6、检查云平台资源保留项（全局设置-基本页面内容）
# 7、每种主存储类型下的物理机数量；
# 8、SAN集群物理机识别的多路径是否相同；
# 9、新增报警项，镜像服务器、主存储保留容量过小，云主机高可用开关，CPU、内存、主存储超分率，物理机保留内存，主存储使用阈值
# 10
# V1.7.0
# 2019-03-19
# 修改碎片化检查大小为100GB-200GB之间的本地存储文件。
# 2019-03-14
# 1 修复了巡检Ceph时可能没有打包日志的错误
# 2 按照模板，重新格式化显示管理节点报告数据
# 3 修复CPU利用率超过80%的排序问题
# 4 修复物理机内存检查显示错误
# 5 修复获取平台信息的错误(修改过mysql密码)



# 2019-03-12
# 1 优化会话数检查
# 2 优化管理节点报告模式
# V 1.6.0
# 2019-03-09
#巡检脚本新增功能：
#（1）管理节点、计算节点生成单独的csv文件，可以用Excel直接打开
#（2）检查管理节点prometheus的监控数据占用空间的大小，若大于120GB，则报警
#（3）增加Ceph安装路径占用空间检查（/opt/sds）
#（4）按照姜翀生成Word的需求，修改数据输出格式
#（5）新增物理机检查超时自动停止巡检并提示异常
#（6）新增管理节点与企业版Ceph共用时，交互提示用户是否同时做企业版Ceph的巡检，并优化了巡检提示信息
# 巡检脚本修复的问题：
#（1）修复了getInfo.sh中的错误，优化执行的速度
#（2）修复了物理机检查过程hang住的问题
#（3）修复了Ceph检查可能hang住的问题
#（4）修复巡检报告镜像服务器信息显示不正确的问题
#（5）修复镜像服务器名称中空格可能导致格式错误
# 2019-03-08
# 1、修复巡检报告镜像服务器信息显示不正确的问题；
# 2、新增物理机检查超时自动停止巡检并提示异常(900s)
# 2019-03-07
# 优化检查云盘信息的速度
# 修复Ceph巡检问题
# 2019-03-06
# 生成管理节点报告
# 2019-03-05
# 更新了 getInfo.sh
# 2019-03-04
#（1）加入系统软件的安装包检查，非ZStack的软件包  -- 拉取已安装的rpm包
#（2）加入系统关键组件的额外配置检查，sysctl -p的检查
#（3）修复了rc.local可执行权限检查存在的问题
#（4）巡检 zstack.properties、/opt/zstack-dvd/.repo_version到巡检结果，判断按的ZStack的repo版本
#（5）网络规划检查，二层网络名称 VLANID   物理设备接口 三层网络 网络段  公有 私有 系统
#（6）全局设置增加通用项检查，含中文解释
#（7）各节点系统盘容量/分区小于300G，报警
#（8）增加了ZStack企业版Ceph检查的新功能
#（9）新增了结果生成csv格式报告（编写中）
#（10）修复了占用26个session的问题
# ..............


# V1.5.2
# 2018-12-23
# 1、修改mysql使用zstack用户登录查询；
# 2、修复ansible运行时ssh需输入yes；
# 3、修复获取rc.local信息不正确的问题。

# V1.5.1
# 2018-12-19
# 1、巡检脚本生成表格形式，隐藏详细信息，只输出巡检的条目，条目信息包含巡检的项目，成功还是失败，失败应该报错
# 2、chrony时间同步异常告警
# 3、巡检 后续客户的请求码和license许可证书 看起来也都收集回来。省得到时候找不来。
# 4、每个物理上各虚拟机规格；host日志
# 5. 物理机上CPU 超线程后总量+内存
# 6. 物理机上云主机的数量 和使用的虚拟CPU数量总和+1127状态
# 7. 物理机上内存总量和每个虚拟机进程消耗的内存总和 及真实空闲
# 8、巡检脚本包括人工巡检界面展示信息 MN 日志
# 9、V2V
# 10、Baremetal
# 11、 灾备
# 12、全局设置的基础设置 查询
# 13、网卡工作的速度
# 13、网卡是否存在错误包
# 15、数据库备份文件大小
# 16、日志文件大小
# 17、监控文件大小 prometheus
# 18、审计文件大小 influxdb
# 19、是否超融合部署 管理节点是否独立部署 统计有误，需重新写逻辑
# 20、运行完成后删除hardware.pl脚本
# 21、巡检脚本需输入密码执行
# 22、/var/log大小
# 23、巡检加入物理机时区检查，
# 24、加上物理机dmidecode相关信息获取相关信息，
# 25、加入ethtool对物理网卡带宽 驱动 固件的检查
# 26、加入对GPU设备检查，USB设备检查。
# 27、云平台检查客户是否使用gpu透传，usb透传
# 28、检查所有节点时区一致性（默认东八区）
# 29、检查所有节点指向同一时间同步源
# 30、/etc/rc.local文件应拥有可执行权限
# 31、检查镜像仓库的目录开机应自动挂载相应的硬盘
# 32、检查本地主存储的目录开机应自动挂载相应的硬盘
# 33、检查Ceph存储应关闭swap分区
# 34、检查云平台界面的物理机内存保留设置（建议4G起，需额外考虑存储占用）
# 35、重写研发获取信息需求
# 36、增加进度条显示
# 37、巡检时刷新云盘容量
# 38、获取计算节点的CPU信息成一个单独的文件
# 39、注释了关于自动创建备份任务的计划任务
# 40、修改日志目录到tmp，修改了相关删除、备份策略
# 41、修复了检查物理机可能报错的问题
# 42、修复了检查VM时可能卡的问题
# 43、删除收集message
# 44、数据库备份显示信息不正确
# 45、检查单独管理节点
# 46、ceph postgresql进程数检查
# 47、检查每个云主机的云盘数量及快照数量（状态为Ready的云盘）
# 48、修复低版本无法获取首页数据的问题
# 49、修复Vcenter数据获取错误的问题，注释获取首页数据问题
# 50、添加查询bond信息
# V1.5.0.2
# 2018-11-23
# 1、chrony时间同步异常告警;
# 2、收集客户的请求码和license许可证书;
# 3、每个物理上各虚拟机规格；host日志;
# 4、物理机上云主机的数量和使用的虚拟CPU数量总和;
# 5、运行完成后删除hardware.pl脚本；
# 6、隐藏了检查物理机信息时ansible的执行输出。
# V1.5.0.1
# 2018-10-26
# 1、增加密码检查功能，若密码错误，会有友好提示语并退出;
# 2、
# V1.4.5
# 1、qemu进程是否在virsh-list中;
# 2、是否有vm运行在多个物理机上;
# 3、检查xdc、xms版本。
# V1.4.4
# 增加了平台基本信息检查，如vCenter主机的个数、vCenter云主机的个数(见check_mn.sh)
# V1.4.3
# 1、增加了Raid信息检查;
# 2、增加了San存储信息检查;
# 3、增加了检查物理机上virsh list、qemu-kvm进程的云主机UUID信息，并统计相关信息。
# V1.4.2
# 1、增加了session数检查，若平台会话数不足以支持巡检正常进行，则退出，并报错；
# 2、修改了获取平台信息的脚本，可自动将platform_info.csv转换为ANSI编码。
# V1.4.1
# 2018-09-19
# excel增加标题行
# V1.4
# 2018-09-14
# 1、删除了将日志生成excel的功能；
# 2、增加了检查磁盘IO、网络IO功能(瞬时数据);
# 3、增加了获取平台信息的功能(getInfo.sh)，共检测28项信息，并生成Excel表格，检测结果存放于log/platform_info.csv;
# 4、优化了zsha检测功能。

#V1.2
# 2018-08-16
# 1、删除了巡检脚本中巡检结束前清空session的语句（xunjian.sh）；
# 2、增加检查mysql备份文件是否存在，包括本地备份和远程备份（zs_mn_TSC.py）；

# V1.1
# 2018-08-10
# 更新日志：
# 1、增加硬件检测（新增[perl]hardware.pl）
# 2、增加检测zstack、zsha、zsha2状态（zs_mn_TSC.py）
# 4、管理节点日志名称由MN_log更换为management.log(xunjian.sh zs_check.py zs_check.sh zs_mn_TSC.py)
# 5、日志导出到excel(新增log_to_excel.py,需安装相应的xlwt模块)
# 6、virsh version获取libvirt  qemu版本（更新zs_check.py）


